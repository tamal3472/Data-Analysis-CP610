{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39bba82",
   "metadata": {},
   "source": [
    "## <center> Assignment 2 </center>\n",
    "\n",
    "#### Name: Tamal Chakroborty\n",
    "#### Student ID:245830440\n",
    "\n",
    "You are provided with a training dataset and a testing dataset for a binary classification problem with labels {0,1}. The last column of the training set is the label, while the test dataset contains only attributes.\n",
    "\n",
    "Train an effective classifier using the training dataset. You are free to choose your data processing approach, the classifier type, and tune the classifier's parameters as needed. You can use the sklearn package in Python for model implementation. \n",
    "\n",
    "Make predictions on the testing dataset and generate a file containing only one column of labels (predicted 0 or 1), in the same order as the testing dataset.\n",
    "\n",
    "Please submit your implementation code and the predicted output file as two separate files (not in a zip) in the names \"A2.ipynb\" and \"prediction.txt\". Your assignment will be evaluated based on the performance of your model, specifically its F1-score, among other criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "2002f4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('A2_data/train.csv',sep=',',index_col=0) \n",
    "df_test_attribute_only = pd.read_csv('A2_data/test_attribute.csv',sep=',',index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "71b2a48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3    4    5     6     7  8\n",
       "233  0.28  0.48  0.36  0.16  0.5  0.0  0.53  0.22  0\n",
       "537  0.53  0.53  0.60  0.13  0.5  0.0  0.49  0.22  0\n",
       "387  0.35  0.21  1.00  0.80  0.5  0.0  0.13  0.01  0\n",
       "320  0.19  0.41  0.55  0.13  0.5  0.0  0.52  0.25  0\n",
       "169  0.45  0.40  0.50  0.16  0.5  0.0  0.50  0.22  0"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## your code here\n",
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd990b-631f-4be7-a3c7-1212c100704d",
   "metadata": {},
   "source": [
    "## To know the details of the data, we will check if there are any missing values, the range of the data, insignificant columns, and class blance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "ec7d20eb-36e9-40c0-a084-aa9493628296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in Each Column:\n",
      " {'0': 0, '1': 0, '2': 0, '3': 0, '4': 0, '5': 0, '6': 0, '7': 0, '8': 0}\n",
      "\n",
      "Range of Each Column:\n",
      " {'0': 0.73, '1': 0.86, '2': 0.79, '3': 0.8, '4': 0.5, '5': 0.83, '6': 0.59, '7': 0.73, '8': 1}\n",
      "\n",
      "Number of Outliers in Each Column:\n",
      " {'0': 10, '1': 20, '2': 11, '3': 25, '4': 6, '5': 8, '6': 25, '7': 86, '8': 64}\n",
      "\n",
      "Frequency of Unique Values in Each Column:\n",
      "\n",
      "Frequency counts for column '0':\n",
      "0\n",
      "0.51    32\n",
      "0.46    31\n",
      "0.45    29\n",
      "0.50    25\n",
      "0.47    23\n",
      "        ..\n",
      "0.87     1\n",
      "0.83     1\n",
      "0.88     1\n",
      "0.29     1\n",
      "0.24     1\n",
      "Name: count, Length: 70, dtype: int64\n",
      "\n",
      "Frequency counts for column '1':\n",
      "1\n",
      "0.46    32\n",
      "0.45    32\n",
      "0.48    28\n",
      "0.53    24\n",
      "0.51    23\n",
      "        ..\n",
      "0.78     1\n",
      "0.14     1\n",
      "0.18     1\n",
      "1.00     1\n",
      "0.80     1\n",
      "Name: count, Length: 68, dtype: int64\n",
      "\n",
      "Frequency counts for column '2':\n",
      "2\n",
      "0.54    42\n",
      "0.53    40\n",
      "0.51    39\n",
      "0.52    37\n",
      "0.50    35\n",
      "0.55    31\n",
      "0.56    29\n",
      "0.49    29\n",
      "0.48    26\n",
      "0.47    25\n",
      "0.45    24\n",
      "0.57    23\n",
      "0.46    22\n",
      "0.58    20\n",
      "0.43    15\n",
      "0.35    14\n",
      "0.34    14\n",
      "0.38    14\n",
      "0.36    14\n",
      "0.44    13\n",
      "0.42    13\n",
      "0.59    11\n",
      "0.60    10\n",
      "0.30     9\n",
      "0.32     9\n",
      "0.41     9\n",
      "0.62     8\n",
      "0.33     8\n",
      "0.61     8\n",
      "0.39     7\n",
      "0.63     7\n",
      "0.64     7\n",
      "0.40     7\n",
      "0.37     5\n",
      "0.71     3\n",
      "0.67     3\n",
      "0.29     3\n",
      "0.75     2\n",
      "0.31     2\n",
      "0.21     2\n",
      "0.65     2\n",
      "0.69     2\n",
      "0.26     2\n",
      "0.24     1\n",
      "0.66     1\n",
      "1.00     1\n",
      "0.22     1\n",
      "0.72     1\n",
      "0.27     1\n",
      "0.28     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Frequency counts for column '3':\n",
      "3\n",
      "0.18    38\n",
      "0.19    32\n",
      "0.17    32\n",
      "0.16    30\n",
      "0.22    25\n",
      "        ..\n",
      "0.69     1\n",
      "0.59     1\n",
      "0.75     1\n",
      "0.71     1\n",
      "0.07     1\n",
      "Name: count, Length: 65, dtype: int64\n",
      "\n",
      "Frequency counts for column '4':\n",
      "4\n",
      "0.5    646\n",
      "1.0      6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Frequency counts for column '5':\n",
      "5\n",
      "0.00    644\n",
      "0.83      6\n",
      "0.50      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Frequency counts for column '6':\n",
      "6\n",
      "0.51    67\n",
      "0.53    66\n",
      "0.52    62\n",
      "0.54    60\n",
      "0.50    55\n",
      "0.48    46\n",
      "0.49    46\n",
      "0.55    42\n",
      "0.47    25\n",
      "0.46    25\n",
      "0.56    21\n",
      "0.44    21\n",
      "0.57    21\n",
      "0.45    11\n",
      "0.59     9\n",
      "0.43     9\n",
      "0.40     9\n",
      "0.58     8\n",
      "0.60     6\n",
      "0.41     6\n",
      "0.38     6\n",
      "0.42     5\n",
      "0.36     5\n",
      "0.39     4\n",
      "0.34     3\n",
      "0.62     3\n",
      "0.30     3\n",
      "0.32     1\n",
      "0.37     1\n",
      "0.69     1\n",
      "0.33     1\n",
      "0.27     1\n",
      "0.13     1\n",
      "0.72     1\n",
      "0.26     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Frequency counts for column '7':\n",
      "7\n",
      "0.22    383\n",
      "0.27     42\n",
      "0.25     33\n",
      "0.26     26\n",
      "0.31     25\n",
      "0.11     16\n",
      "0.28     14\n",
      "0.30     10\n",
      "0.40      6\n",
      "0.32      6\n",
      "0.36      6\n",
      "0.29      6\n",
      "0.42      6\n",
      "0.37      6\n",
      "0.34      5\n",
      "0.16      5\n",
      "0.33      5\n",
      "0.43      5\n",
      "0.47      4\n",
      "0.39      4\n",
      "0.44      4\n",
      "0.38      4\n",
      "0.35      3\n",
      "0.41      3\n",
      "0.14      2\n",
      "0.21      2\n",
      "0.54      2\n",
      "0.23      2\n",
      "0.45      1\n",
      "0.66      1\n",
      "0.18      1\n",
      "0.64      1\n",
      "0.73      1\n",
      "0.49      1\n",
      "0.19      1\n",
      "0.57      1\n",
      "0.01      1\n",
      "0.68      1\n",
      "0.50      1\n",
      "0.58      1\n",
      "0.74      1\n",
      "0.70      1\n",
      "0.48      1\n",
      "0.63      1\n",
      "0.65      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Frequency counts for column '8':\n",
      "8\n",
      "0    588\n",
      "1     64\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = {}\n",
    "ranges = {}\n",
    "outlier_info = {}\n",
    "frequency_counts = {}\n",
    "\n",
    "# Loop through each column\n",
    "for column in df_train.columns:\n",
    "    # Step 1: Count missing values\n",
    "    missing_values[column] = df_train[column].isnull().sum()\n",
    "    \n",
    "    # Step 2: Calculate range (max - min)\n",
    "    ranges[column] = df_train[column].max() - df_train[column].min()\n",
    "    \n",
    "    # Step 3: Outliers detection using IQR\n",
    "    Q1 = df_train[column].quantile(0.25)\n",
    "    Q3 = df_train[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df_train[(df_train[column] < lower_bound) | (df_train[column] > upper_bound)]\n",
    "    outlier_info[column] = len(outliers)\n",
    "    frequency_counts[column] = df_train[column].value_counts()\n",
    "\n",
    "\n",
    "# Display the results\n",
    "print(\"Missing Values in Each Column:\\n\", missing_values)\n",
    "print(\"\\nRange of Each Column:\\n\", ranges)\n",
    "print(\"\\nNumber of Outliers in Each Column:\\n\", outlier_info)\n",
    "print(\"\\nFrequency of Unique Values in Each Column:\")\n",
    "\n",
    "# Print frequency counts for each column\n",
    "for column, counts in frequency_counts.items():\n",
    "    print(f\"\\nFrequency counts for column '{column}':\\n{counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01339c6-4f6c-48d5-b4b5-2336d036f372",
   "metadata": {},
   "source": [
    "#### From the data it is evident that there are outliers in the data and the classes are imbalance. Additionally, column 4 insignificant.\n",
    "\n",
    "**To address outliers:** the tree based model(e.g., Random Forests, Decision Trees) can be better than distance based model for the data and we can use some approaches(e.g. Robust Scaler) to handle the outliers.\n",
    "\n",
    "**To address class imbalance:** We have to use class weightining factors in the model and if it does not perform well enough we need to resample to balance the classes.\n",
    "\n",
    "**Insignificant Columns:** Since column 4 is statistically insignificant, we can check the accuracy removing it and keeping it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8dff8-25e6-4f30-a762-d9620ca11785",
   "metadata": {},
   "source": [
    "# Data preprocessing by removing columns and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "61ea2e9b-8cad-4179-bf23-9801bd9c9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "class_labels = df_train.iloc[:, -1]\n",
    "\n",
    "\n",
    "## Removing column\n",
    "filtered_df_train = df_train.drop(df_train.columns[[4]], axis=1)\n",
    "filtered_features = filtered_df_train.iloc[:, :-1]\n",
    "class_labels = filtered_df_train.iloc[:, -1]\n",
    "\n",
    "filtered_X_train, filtered_X_test, filtered_y_train, filtered_y_test = train_test_split(filtered_features, class_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit on the training data\n",
    "filtered_X_train_scaled = pd.DataFrame(scaler.fit_transform(filtered_X_train), columns=filtered_X_train.columns)\n",
    "filtered_X_test_scaled = pd.DataFrame(scaler.transform(filtered_X_test), columns=filtered_X_test.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## without Removing column\n",
    "features = df_train.iloc[:, :-1]\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, class_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit on the training data\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c568875-06df-4057-a2b6-eff2f3503c34",
   "metadata": {},
   "source": [
    "## We will test with Random forest and Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "c6578130-dd13-46dd-bf6f-895fc3bf97cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered scaled data -> F1-score for random forest: 0.961255898604444\n",
      "Filtered scaled data -> F1-score for logistic regression: 0.9240248677485524\n",
      "Unfiltered scaled data -> F1-score for random forest: 0.9694656488549618\n",
      "Unfiltered scaled data -> F1-score for logistic regression: 0.9694656488549618\n"
     ]
    }
   ],
   "source": [
    "model_rf = RandomForestClassifier(class_weight='balanced',random_state=42)\n",
    "model_lr = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "\n",
    "\n",
    "# For filtered scaled data\n",
    "\n",
    "# Random forest\n",
    "model_rf.fit(filtered_X_train_scaled, filtered_y_train)\n",
    "filtered_y_pred = model_rf.predict(filtered_X_test_scaled)\n",
    "filtered_f1_RF = f1_score(filtered_y_test, filtered_y_pred, average='weighted')\n",
    "print(\"Filtered scaled data -> F1-score for random forest:\", filtered_f1_RF)\n",
    "\n",
    "# Logistic regression\n",
    "model_lr.fit(filtered_X_train_scaled, filtered_y_train)\n",
    "filtered_y_pred = model_lr.predict(filtered_X_test_scaled)\n",
    "filtered_f1_LR = f1_score(filtered_y_test, filtered_y_pred, average='weighted')\n",
    "print(\"Filtered scaled data -> F1-score for logistic regression:\", filtered_f1_LR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For unfiltered scaled data\n",
    "\n",
    "# Random forest\n",
    "model_rf.fit(X_train_scaled, y_train)\n",
    "y_pred = model_rf.predict(X_test_scaled)\n",
    "f1_RF = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Unfiltered scaled data -> F1-score for random forest:\", f1)\n",
    "\n",
    "# Logistic regression\n",
    "model_lr.fit(X_train_scaled, y_train)\n",
    "y_pred = model_lr.predict(X_test_scaled)\n",
    "f1_LR = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Unfiltered scaled data -> F1-score for logistic regression:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80bcd0-d0ab-4512-bade-1ec842958d90",
   "metadata": {},
   "source": [
    "### It seems the filtering(removing insignificant column) has no effect or adverse effect on the classification. Additionally,  random forest is performing better in both of the cases.\n",
    "\n",
    "**We can select the Random forest for prediction. Since the accuracy is not very good we can check with the SMOTE(Synthetic Minority Oversampling) to balance out the imbalance data** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "673dee28-e291-4d15-9fc0-814cb24c3563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification detais report of Random forest on unfiltered data with SMOTE \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       116\n",
      "           1       0.87      0.87      0.87        15\n",
      "\n",
      "    accuracy                           0.97       131\n",
      "   macro avg       0.92      0.92      0.92       131\n",
      "weighted avg       0.97      0.97      0.97       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Train on the resampled data\n",
    "model_rf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_rf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print('Classification detais report of Random forest on unfiltered data with SMOTE \\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2772b0-d134-48d3-98e6-aad106900436",
   "metadata": {},
   "source": [
    "### Since the accuracy got improved, we can generate prodiction  for the given test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "76aec7da-8f5f-4224-ba24-89f38d7fa07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_attribute_only_scaled = pd.DataFrame(scaler.transform(df_test_attribute_only), columns=df_test_attribute_only.columns)\n",
    "\n",
    "\n",
    "test_attribute_prediction = model_rf.predict(df_test_attribute_only_scaled)\n",
    "\n",
    "pd.DataFrame(test_attribute_prediction, columns=['Prediction']).to_csv(\"prediction.txt\", header= True, index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea59e06",
   "metadata": {},
   "source": [
    "### Briefly describe your approach in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e0b3d9-40cd-4671-96c6-92dc14ffd240",
   "metadata": {},
   "source": [
    "#### First, we analyzed the data and considered attribute selection and preprocessing stretegies as explained above. Additionally, based on the data we decided the suitable classifiers and sampling requirements. Each of the steps have been decribed with reasoning above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6531ff0-1a1b-4728-aa51-7a3da6cc5c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
